{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyeeg as pe\n",
    "import pickle as pickle\n",
    "import pandas as pd\n",
    "import math\n",
    "import mne\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "channel = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18] #14 Channels chosen to fit Emotiv Epoch+\n",
    "band = [4,8,12,16,25,45] #5 bands\n",
    "window_size = 2 #Averaging band power of 2 sec\n",
    "step_size = 0.1 #Each 0.1 sec update once\n",
    "sample_rate = 500 #Sampling rate of 128 Hz\n",
    "subjectList = ['01','02','03']\n",
    "#List of subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from tensorflow.python.keras.layers.convolutional import Conv1D\n",
    "from tensorflow.python.keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical \n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "import timeit\n",
    "from keras.models import Sequential\n",
    "from tensorflow.python.keras.layers.core import Flatten, Dense, Dropout\n",
    "from tensorflow.python.keras.layers.convolutional import Convolution1D, MaxPooling1D, ZeroPadding1D\n",
    "from keras.optimizers import SGD\n",
    "#import cv2, numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "raw = mne.io.read_raw_eeglab(\"C:\\Project_summary\\EGGDataSet\\EGGRealDataSet\\sub-001\\eeg\\sub-001_task-eyesclosed_eeg.set\",preload=False)\n",
    "data,times=raw[:,:]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFT_Processing (sub, channel, band, window_size, step_size, sample_rate):\n",
    "    '''\n",
    "    arguments:  string subject#绝对路径,包含participants.tsv和derivative文件夹\n",
    "                list channel indice\n",
    "                list band\n",
    "                int window size for FFT\n",
    "                int step size for FFT\n",
    "                int sample rate for FFT\n",
    "    return:     void\n",
    "    '''\n",
    "    # meta = []\n",
    "    #with open(\"/content/drive/My Drive/leading_ai/try/s\" + sub + '.dat', 'rb') as file:\n",
    "    # raw = mne.io.read_raw_eeglab(\"C:/Project_summary/EGGDataSet/EGGRealDataSet/derivatives/sub-001/eeg/sub-001_task-eyesclosed_eeg.set\",preload=False)\n",
    "    # if raw is not None :\n",
    "    #     #subject = pickle.load(file, encoding='latin1') #resolve the python 2 data problem by encoding : latin1\n",
    "\n",
    "    #     for i in range (0,40):\n",
    "    #         # loop over 0-39 trails\n",
    "    #         # data = subject[\"data\"][i]#读取文件中的单个样本\n",
    "    #         # labels = subject[\"labels\"][i]#读取样本标签\n",
    "    #         start = 0\n",
    "\n",
    "    #         while start + window_size < raw[0][0].shape[1]:\n",
    "    #             meta_array = []\n",
    "    #             meta_data = [] #meta vector for analysis\n",
    "    #             for j in channel:#读取不同通道\n",
    "    #                 X = raw[j][0][start : start + window_size]\n",
    "    #                 # X = data[j][start : start + window_size] #Slice raw data over 2 sec, at interval of 0.125 sec\n",
    "    #                 Y = pe.bin_power(X, band, sample_rate) #FFT over 2 sec of channel j, in seq of theta, alpha, low beta, high beta, gamma\n",
    "    #                 meta_data = meta_data + list(Y[0])\n",
    "\n",
    "    #             meta_array.append(np.array(meta_data))\n",
    "    #             meta_array.append(labels)\n",
    "\n",
    "    #             meta.append(np.array(meta_array))    \n",
    "    #             start = start + step_size\n",
    "                \n",
    "    #     meta = np.array(meta)\n",
    "    #np.save('/content/drive/My Drive/leading_ai/try/s' + sub, meta, allow_pickle=True, fix_imports=True)\n",
    "    meta = []\n",
    "    df = pd.read_csv(sub+'participants.tsv', delimiter = '\\t')\n",
    "\n",
    "    # LabelDict={}\n",
    "    # NumberOfData = len(df)\n",
    "    # for k in range(NumberOfData):\n",
    "    #     label = df.iloc[k,-2]\n",
    "    #     if label not in LabelDict:\n",
    "    #         LabelDict[label]=len(LabelDict)\n",
    "    # print(LabelDict)\n",
    "\n",
    "    for i in range(NumberOfData):\n",
    "        print(i)\n",
    "        raw = mne.io.read_raw_eeglab(sub+\"derivatives/sub-{:03d}/eeg/sub-{:03d}_task-eyesclosed_eeg.set\".format(i+1,i+1),preload=False)\n",
    "        sfreq=raw.info['sfreq']\n",
    "        data,_= raw[:,:]\n",
    "        label = df.iloc[i,-2]\n",
    "        start = 0\n",
    "        while start + window_size*sfreq < len(data[0]):\n",
    "            meta_array = []\n",
    "            meta_data = [] #meta vector for analysis\n",
    "            for j in channel:#读取不同通道 \n",
    "                ChannelData = data[j]\n",
    "                # print(ChannelData)\n",
    "                # print(start, start + window_size*sfreq)\n",
    "                X = ChannelData[int(start) : int(start + window_size*sfreq)] #Slice raw data over 2 sec, at interval of 0.125 sec\n",
    "                Y = pe.bin_power(X, band, sample_rate) #FFT over 2 sec of channel j, in seq of theta, alpha, low beta, high beta, gamma\n",
    "                meta_data = meta_data + list(Y[0])\n",
    "            meta_array.append(np.array(meta_data))\n",
    "            # labelCode = np.zeros(len(LabelDict))\n",
    "            # labelCode[LabelDict[label]] = 1 \n",
    "            # meta_array.append(labelCode)\n",
    "            meta_array.append(label)\n",
    "            meta.append(np.array(meta_array))    \n",
    "            start = start + step_size * sfreq\n",
    "    meta = np.array(meta)\n",
    "    np.save('./nparray/meta.npy', meta, allow_pickle=True, fix_imports=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFT_Processing(\"C:/Project_summary/EGGDataSet/EGGRealDataSet/\", channel=channel, band=band, window_size=window_size, step_size=step_size, sample_rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training = []\n",
    "label_training = []\n",
    "data_testing = []\n",
    "label_testing = []\n",
    "\n",
    "\n",
    "\n",
    "with open('./nparray/meta.npy', 'rb') as file:\n",
    "    sub = np.load(file,allow_pickle=True)\n",
    "    for i in range (0,sub.shape[0]):\n",
    "        if i % 5 == 0:\n",
    "          data_testing.append(sub[i][0])\n",
    "          label_testing.append(sub[i][1])\n",
    "        else:\n",
    "          data_training.append(sub[i][0])\n",
    "          label_training.append(sub[i][1])\n",
    "\n",
    "\n",
    "np.save('./DataAndLabels/data_training', np.array(data_training), allow_pickle=True, fix_imports=True)\n",
    "np.save('./DataAndLabels/label_training', np.array(label_training), allow_pickle=True, fix_imports=True)\n",
    "print(\"training dataset:\", np.array(data_training).shape, np.array(label_training).shape)\n",
    "\n",
    "np.save('./DataAndLabels/data_testing', np.array(data_testing), allow_pickle=True, fix_imports=True)\n",
    "np.save('./DataAndLabels/label_testing', np.array(label_testing), allow_pickle=True, fix_imports=True)\n",
    "print(\"testing dataset:\", np.array(data_testing).shape, np.array(label_testing).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14718135 0.06269111 0.05242517 ... 0.04258694 0.0520913  0.09663831]\n",
      " [0.12287107 0.07193411 0.05394435 ... 0.03949061 0.06694669 0.10783583]\n",
      " [0.11980121 0.06977577 0.05594874 ... 0.03924718 0.07156676 0.10738157]\n",
      " ...\n",
      " [0.10570603 0.07686313 0.04531252 ... 0.04259257 0.08191864 0.07792404]\n",
      " [0.12760023 0.08765102 0.04443266 ... 0.03925141 0.07168894 0.07462111]\n",
      " [0.13554783 0.07468287 0.02809172 ... 0.03004628 0.07509905 0.07695327]] [[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "with open('./DataAndLabels/data_training.npy', 'rb') as fileTrain:\n",
    "    X  = np.load(fileTrain)\n",
    "    \n",
    "with open('./DataAndLabels/label_training.npy', 'rb') as fileTrainL:\n",
    "    Y  = np.load(fileTrainL)\n",
    "    \n",
    "X = normalize(X)\n",
    "Z = Y\n",
    "print(X,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import to_categorical\n",
    "# y_train = to_categorical(Z)\n",
    "y_train = np.array(Z)\n",
    "x_train = np.array(X[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11410238 0.08058227 0.06283299 ... 0.04662829 0.07533678 0.10683679]\n",
      " [0.1062635  0.08320407 0.06757253 ... 0.04018427 0.06642195 0.10408628]\n",
      " [0.15842044 0.08599591 0.07072215 ... 0.06662579 0.08338724 0.10151775]\n",
      " ...\n",
      " [0.13567688 0.0692273  0.05036737 ... 0.05641399 0.0773523  0.07916984]\n",
      " [0.12497286 0.060154   0.05001043 ... 0.05833618 0.06855889 0.08201546]\n",
      " [0.11371868 0.07288457 0.04826057 ... 0.04809681 0.07477087 0.07914009]] [[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "with open('./DataAndLabels/data_testing.npy', 'rb') as fileTrain:\n",
    "    M  = np.load(fileTrain)\n",
    "    \n",
    "with open('./DataAndLabels//label_testing.npy', 'rb') as fileTrainL:\n",
    "    N  = np.load(fileTrainL)\n",
    "\n",
    "M = normalize(M)\n",
    "L = N\n",
    "print(M,L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(M[:])\n",
    "y_test = np.array(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0],x_train.shape[1], 1)\n",
    "x_test = x_test.reshape(x_test.shape[0],x_test.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(557260, 95, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_classes = 3\n",
    "epochs = 200\n",
    "input_shape=(1, x_train.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 95, 1)\n"
     ]
    }
   ],
   "source": [
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Convolution1D, ZeroPadding1D, MaxPooling1D, BatchNormalization, Activation, Dropout, Flatten, Dense\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " module_wrapper (ModuleWrapp  (None, 95, 128)          512       \n",
      " er)                                                             \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 95, 128)          512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 47, 128)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " module_wrapper_1 (ModuleWra  (None, 47, 128)          49280     \n",
      " pper)                                                           \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 47, 128)          512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 23, 128)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " module_wrapper_2 (ModuleWra  (None, 23, 64)           24640     \n",
      " pper)                                                           \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 11, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 704)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                45120     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 123,235\n",
      "Trainable params: 122,723\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "input_shape=(None, x_train.shape[1], 1)\n",
    "model.add(Conv1D(128, kernel_size=3,padding = 'same',activation='relu', input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "model.add(Conv1D(128,kernel_size=3,padding = 'same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "model.add(Conv1D(64,kernel_size=3,padding = 'same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.build(input_shape)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,  \n",
    "          verbose=1,validation_data=(x_test,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
